<link rel="stylesheet" href="https://stackpath.bootstrapcdn.com/bootstrap/4.3.1/css/bootstrap.min.css">
<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <!-- Meta tags for social media banners, these should be filled in appropriatly as they are your "business card" -->
  <!-- Replace the content tag with appropriate information -->
  <meta name="description" content="From ViT Features to Training-free Video Object
Segmentation via Streaming-data Mixture Models">
  <meta property="og:title" content="From ViT Features to Training-free Video Object
Segmentation via Streaming-data Mixture Models"/>
  <meta property="og:description" content="From ViT Features to Training-free Video Object
Segmentation via Streaming-data Mixture Models"/>
  <meta property="og:url" content="https://trainingfreevos.github.io/"/>
  <!-- Path to banner image, should be in the path listed below. Optimal dimenssions are 1200X630-->
  <meta property="og:image" content="static/image/your_banner_image.png" />
  <meta property="og:image:width" content="1200"/>
  <meta property="og:image:height" content="630"/>


  <meta name="twitter:title" content="From ViT Features to Training-free Video Object
Segmentation via Streaming-data Mixture Models">
  <meta name="twitter:description" content="From ViT Features to Training-free Video Object
Segmentation via Streaming-data Mixture Models">
  <!-- Path to banner image, should be in the path listed below. Optimal dimenssions are 1200X600-->
  <meta name="twitter:image" content="static/images/your_twitter_banner_image.png">
  <meta name="twitter:card" content="summary_large_image">
  <!-- Keywords for your paper to be indexed by-->
  <meta name="keywords" content="From ViT Features to Training-free Video Object
Segmentation via Streaming-data Mixture Models">
  <meta name="viewport" content="width=device-width, initial-scale=1">


  <title>From ViT Features to Training-free Video Object Segmentation via Streaming-data Mixture Models</title>
  <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro"
  rel="stylesheet">

  <link rel="stylesheet" href="static/css/bulma.min.css">
  <link rel="stylesheet" href="static/css/bulma-carousel.min.css">
  <link rel="stylesheet" href="static/css/bulma-slider.min.css">
  <link rel="stylesheet" href="static/css/fontawesome.all.min.css">
  <link rel="stylesheet"
  href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  <link rel="stylesheet" href="static/css/index.css">

  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script src="https://documentcloud.adobe.com/view-sdk/main.js"></script>
  <script defer src="static/js/fontawesome.all.min.js"></script>
  <script src="static/js/bulma-carousel.min.js"></script>
  <script src="static/js/bulma-slider.min.js"></script>
  <script src="static/js/index.js"></script>
</head>
<body>


  <section class="hero">
    <div class="hero-body">
      <div class="container is-max-desktop">
        <div class="columns is-centered">
          <div class="column has-text-centered">
            <h1 class="title is-1 publication-title">From ViT Features to Training-free Video Object
Segmentation via Streaming-data Mixture Models</h1>
            <div class="is-size-5 publication-authors">
              <!-- Paper authors -->
              <span class="author-block">
                Anonymous Author(s)
   
                  </span>
                  </div>

                  <div class="is-size-5 publication-authors">
                    <span class="author-block">Affiliation<br> </span>
                  </div>

                  <!-- Github link -->
                  <span class="link-block">
                    <a href="https://github.com/YOUR REPO HERE" target="_blank"
                    class="external-link button is-normal is-rounded is-dark">
                    <span class="icon">
                      <i class="fab fa-github"></i>
                    </span>
                    <span>Code</span>
                  </a>
                </span>

            </div>
          </div>
        </div>
      </div>
    </div>
  </div>
</section>


<!-- Teaser video-->
<section class="hero teaser">
  <div class="container is-max-desktop">
    <div class="hero-body">
      <div class="row mb-5"> <!-- Adjust space between rows here -->
        <!-- Video 1 -->
        <div class="col-12 col-sm-6 my-3 px-1"> <!-- Adjust column margins and padding here -->
          <video poster="" id="video1" autoplay controls muted loop class="w-100 h-100">
            <!-- Your video here -->
            <source src="static/videos/output0.mp4" type="video/mp4">
          </video>
        </div>
        <!-- Video 2 -->
        <div class="col-12 col-sm-6 my-3 px-1"> <!-- Adjust column margins and padding here -->
          <video poster="" id="video2" autoplay controls muted loop class="w-100 h-100">
            <!-- Your video here -->
            <source src="static/videos/output1.mp4" type="video/mp4">
          </video>
        </div>
      </div>
      <div class="row">
        <!-- Video 3 -->
        <div class="col-12 col-sm-6 my-3 px-1"> <!-- Adjust column margins and padding here -->
          <video poster="" id="video3" autoplay controls muted loop class="w-100 h-100">
            <!-- Your video here -->
            <source src="static/videos/output2.mp4" type="video/mp4">
          </video>
        </div>
        <!-- Video 4 -->
        <div class="col-12 col-sm-6 my-3 px-1"> <!-- Adjust column margins and padding here -->
          <video poster="" id="video4" autoplay controls muted loop class="w-100 h-100">
            <!-- Your video here -->
            <source src="static/videos/dance-twirl/dance-twirl.mp4" type="video/mp4">
          </video>
        </div>
      </div>
      <h2 class="subtitle has-text-centered">
        caption for the videos
      </h2>
    </div>
  </div>
</section>
<!-- End teaser video -->


<!-- Paper abstract -->
<section class="section hero is-light">
  <div class="container is-max-desktop">
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Abstract</h2>
        <div class="content has-text-justified">
          <p>
In the task of semi-supervised video object segmentation, the input is the binary mask of an object in the first frame, and the desired output consists of the corresponding masks of that object in the subsequent frames. Existing leading solutions have two main drawbacks: 1) an expensive and typically-supervised training on videos; 2) a large memory footprint during inference. Here we present a training-free solution, with a low-memory footprint, that yields state-of-the-art results. The proposed method combines pre-trained deep learning-based features (trained on still images) with more classical methods for streaming-data clustering. Designed to adapt to temporal concept drifts and generalize to diverse video content without relying on annotated images or videos, the method eliminates the need for additional training or fine-tuning, ensuring fast inference and immediate applicability to new videos. Concretely, we represent an object via a dynamic ensemble of temporally- and spatially-coherent mixtures over a representation built from pre-trained ViT features and positional embeddings. A convolutional conditional random field further improves spatial coherence and helps reject outliers. We demonstrate the efficacy of the method on key benchmarks: the DAVIS-2017 and YouTube-VOS 2018 validation datasets. Moreover, by the virtue of the low-memory footprint of the compact cluster-based representation, the method scale gracefully to high-resolution ViT features. Our code will be released upon acceptance.          </p>
        </div>
      </div>
    </div>
  </div>
</section>
<!-- End paper abstract -->



<!-- Statcounter tracking code -->
  
<!-- You can add a tracker to track page visits by creating an account at statcounter.com -->

    <!-- End of Statcounter Code -->

  </body>
  </html>
